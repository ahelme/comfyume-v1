# Serverless Implementation Update

**Project:** ComfyuME
**Date:** 2026-02-03
**Status:** ðŸŸ¢ WORKING - H200 SPOT serverless deployed and tested

---

## Summary

Serverless inference mode with multi-GPU support is **WORKING**! H200 SPOT deployment active and receiving requests. Queue-manager routes to serverless (not instance).

**Key Fixes Applied:**
1. Created Inference API Key (required for auth - was returning 404 without it)
2. Changed CMD from `python` to `python3` (container only has python3)
3. Added `SERVERLESS_API_KEY` to config and docker-compose

---

## Quick Test

```bash
# Test serverless directly
API_KEY="dc_62ed1655..."
curl -H "Authorization: Bearer $API_KEY" https://containers.datacrunch.io/comfyume-vca-ftv-h200-spot/
# Should return ComfyUI HTML (HTTP 200)

# Check queue-manager mode
curl -s http://localhost:3000/health | jq '.inference_mode, .active_gpu'
# Should show: "serverless", "H200-141GB-SPOT"
```

---

## Completed Work

### 1. Code Implementation (Issue #62)

**Files Modified:**
- `queue-manager/config.py` - Multi-endpoint support with 4 GPU options
- `queue-manager/main.py` - Serverless routing, CORS for aiworkshop.art
- `queue-manager/models.py` - HealthCheck includes active_gpu field
- `docker-compose.yml` - Serverless env vars added to queue-manager
- `.env` - All 4 endpoint URLs configured

**New Files:**
- `h200-spot.env` - H200 SPOT config (â‚¬0.97/hr + VAT)
- `h200-on-demand.env` - H200 On-Demand config (â‚¬2.80/hr + VAT)
- `b300-spot.env` - B300 SPOT config (â‚¬1.61/hr + VAT)
- `b300-on-demand.env` - B300 On-Demand config (â‚¬4.63/hr + VAT)
- `scripts/switch-gpu.sh` - CLI tool to switch between GPU modes

**Git Commits:**
- `55337d8` - feat: complete serverless multi-GPU implementation (#62)
- `65e7744` - feat: add H100/B300 spot and on-demand serverless options
- `044b44c` - feat: switch from H100 to H200 for serverless inference

### 2. Pricing Analysis

| Deployment | GPU | Base | +19% VAT | Use Case |
|------------|-----|------|----------|----------|
| comfyume-vca-ftv-h200-spot | H200 141GB | â‚¬0.97/hr | â‚¬1.15/hr | Workshop, testing |
| comfyume-vca-ftv-h200-on-demand | H200 141GB | â‚¬2.80/hr | â‚¬3.33/hr | Important demos |
| comfyume-vca-ftv-b300-spot | B300 288GB | â‚¬1.61/hr | â‚¬1.92/hr | Cheap 4K |
| comfyume-vca-ftv-b300-on-demand | B300 288GB | â‚¬4.63/hr | â‚¬5.51/hr | Premium 4K |

### 3. Cost Estimates Calculated

**Scenario #1: Boss Demo + Independent Play (24hr)**
- H200: ~â‚¬10-15 total (actual GPU time ~4-5 hrs)

**Scenario #2: Workshop (3hr, 20 people)**
- H200: ~â‚¬35-45 total (actual GPU time ~16 hrs across parallel containers)

### 4. Verda Serverless Deployment

**Created:** comfyume-vca-ftv-h200-spot

**Configuration:**
```
Container Image:    ghcr.io/ahelme/comfyume-worker:v0.11.0
Registry:           Public
GPU Type:           H200 SXM5 141GB (SPOT)
Number of GPUs:     1
HTTP Port:          8188
Health Check:       /system_stats (NEEDS FIX - see below)
Min Replicas:       0
Max Replicas:       10
Concurrent Req:     1
Scale-up Delay:     0
Scale-down Delay:   300
Request TTL:        36000
Storage:            SFS mounted at /mnt/sfs
```

**Start Command:**
```bash
python /workspace/ComfyUI/main.py --listen 0.0.0.0 --port 8188 --extra-model-paths-config /mnt/sfs/extra_model_paths.yaml
```

**Environment Variables:**
- `HF_HOME=/mnt/sfs/cache/huggingface`
- `HF_TOKEN=<from .env>`

### 5. SFS Configuration

**Created:** `/mnt/sfs/extra_model_paths.yaml`
```yaml
comfyui:
    base_path: /mnt/sfs/models/shared/
    checkpoints: checkpoints/
    diffusion_models: diffusion_models/
    clip: text_encoders/
    vae: vae/
    loras: loras/
    upscale_models: latent_upscale_models/
    controlnet: controlnet/
```

### 6. Endpoint URL Correction

**Wrong (assumed):** `https://comfyume-vca-ftv-h200-spot.containers.verda.com`
**Correct (actual):** `https://containers.datacrunch.io/comfyume-vca-ftv-h200-spot`

Updated in `.env`.

---

## Issues Resolved

### Issue 1: 404 on Serverless Endpoint
**Root Cause:** Missing Inference API Key
**Fix:** Created API key in Verda Console > Keys > Inference API Keys

### Issue 2: Container CrashLoopBackOff
**Root Cause:** `python` not found (container only has `python3`)
**Fix:** Changed CMD to `python3 /workspace/ComfyUI/main.py ...`

### Issue 3: Queue-manager not using auth
**Root Cause:** Missing `SERVERLESS_API_KEY` in config
**Fix:** Added to config.py, docker-compose.yml, and .env

---

## Current Status: WORKING âœ…

- H200 SPOT: Deployed and responding (â‚¬0.97/hr + VAT)
- Queue-manager: Routing to serverless with auth
- Cold start: ~20 seconds (container was pre-warmed)

---

## Remaining Work

1. âœ… ~~Fix health check path~~ - Done (set to `/`)
2. âœ… ~~Create API key~~ - Done
3. âœ… ~~Fix CMD python â†’ python3~~ - Done
4. âœ… ~~Update queue-manager with auth~~ - Done
5. **Test end-to-end** - Submit workflow via ComfyUI, verify runs on serverless
6. **Create remaining deployments** (when needed):
   - comfyume-vca-ftv-h200-on-demand
   - comfyume-vca-ftv-b300-spot
   - comfyume-vca-ftv-b300-on-demand

---

## How to Switch GPU Modes

```bash
# Check current mode
./scripts/switch-gpu.sh status

# Switch to different modes
./scripts/switch-gpu.sh h200-spot       # Workshop (cheapest)
./scripts/switch-gpu.sh h200-on-demand  # Demos (guaranteed)
./scripts/switch-gpu.sh b300-spot       # 4K cheap
./scripts/switch-gpu.sh b300-on-demand  # 4K premium
./scripts/switch-gpu.sh local           # Local/Redis workers

# Apply change
docker compose restart queue-manager
```

---

## Files to Reference

- **Switch script:** `scripts/switch-gpu.sh`
- **Env configs:** `h200-spot.env`, `h200-on-demand.env`, `b300-spot.env`, `b300-on-demand.env`
- **Main config:** `.env` (SERVERLESS_* variables)
- **SFS config:** `/mnt/sfs/extra_model_paths.yaml`
- **GitHub Issue:** #62 (Implement Serverless Inference Mode Switch)

---

## Important Notes

1. **Serverless containers are separate from Verda instance** - The RTX 6000 Ada instance (verda) runs independently
2. **Models on SFS** - All models at `/mnt/sfs/models/shared/` are accessible to both instance and serverless
3. **Flux workflows need special loaders** - Use UNETLoader + DualCLIPLoader + VAELoader (not CheckpointLoaderSimple)
4. **Cold start ~1-2 min** - First request after scale-to-zero takes time

---

---

## Verda Console Deployment Steps (Manual Work)

This section documents the manual configuration done in Verda Console (not in code).

### Step 1: Create Serverless Container Deployment

**Location:** Verda Console > Containers > Create Deployment

**Settings configured:**
- **Name:** `comfyume-vca-ftv-h200-spot`
- **Container Image:** `ghcr.io/ahelme/comfyume-worker:v0.11.0`
- **Registry:** Public (no auth needed)
- **GPU Type:** H200 SXM5 141GB
- **Pricing:** SPOT (â‚¬0.97/hr + VAT)
- **Number of GPUs:** 1
- **HTTP Port:** 8188
- **Health Check Path:** `/` (changed from `/system_stats` which didn't work)
- **Min Replicas:** 0 (scales to zero when idle)
- **Max Replicas:** 10
- **Concurrent Requests per Replica:** 1
- **Scale-up Delay:** 0 seconds
- **Scale-down Delay:** 300 seconds (5 min idle before scale down)
- **Request TTL:** 36000 seconds (10 hours max request time)

### Step 2: Configure Storage Mount

**Location:** Deployment settings > Storage

**SFS Mount:**
- **SFS Volume:** (existing SFS with models)
- **Mount Path:** `/mnt/sfs`
- **Access:** Read-only sufficient (models don't need write)

### Step 3: Set Start Command

**Location:** Deployment settings > Start Command

```bash
python3 /workspace/ComfyUI/main.py --listen 0.0.0.0 --port 8188 --extra-model-paths-config /mnt/sfs/extra_model_paths.yaml
```

**Note:** Must use `python3` not `python` - the container only has python3 binary.

### Step 4: Configure Environment Variables

**Location:** Deployment settings > Environment Variables

| Variable | Value | Purpose |
|----------|-------|---------|
| `HF_HOME` | `/mnt/sfs/cache/huggingface` | HuggingFace cache location |
| `HF_TOKEN` | `hf_xxx...` | HuggingFace auth for gated models |

### Step 5: Create Inference API Key

**Location:** Verda Console > Keys > Inference API Keys > Create

- **Name:** `comfyume-inference`
- **Result:** Key starting with `dc_62ed1655...`

**Critical:** This key is REQUIRED for all requests to the serverless endpoint. Without it, requests return 404.

### Step 6: Deploy and Verify

1. Click "Deploy" in Verda Console
2. Wait for deployment to show "Running" status
3. Test with curl:
   ```bash
   curl -H "Authorization: Bearer dc_62ed1655..." \
     https://containers.datacrunch.io/comfyume-vca-ftv-h200-spot/
   ```
4. Should return ComfyUI HTML (HTTP 200)

### Step 7: Configure Queue-Manager (Code Side)

Added to `.env`:
```
INFERENCE_MODE=serverless
SERVERLESS_ACTIVE=h200-spot
SERVERLESS_ENDPOINT_H200_SPOT=https://containers.datacrunch.io/comfyume-vca-ftv-h200-spot
SERVERLESS_API_KEY=dc_62ed1655...
```

Then restart queue-manager:
```bash
docker compose restart queue-manager
```

### SFS Model Paths Configuration

**File created on SFS:** `/mnt/sfs/extra_model_paths.yaml`

```yaml
comfyui:
    base_path: /mnt/sfs/models/shared/
    checkpoints: checkpoints/
    diffusion_models: diffusion_models/
    clip: text_encoders/
    vae: vae/
    loras: loras/
    upscale_models: latent_upscale_models/
    controlnet: controlnet/
```

This tells ComfyUI where to find models on the mounted SFS volume.

---

## Troubleshooting Reference

| Symptom | Cause | Fix |
|---------|-------|-----|
| 404 on endpoint | Missing API key | Create Inference API Key in Verda Console |
| Container crash loop | `python` not found | Change CMD to use `python3` |
| Models not found | Missing extra_model_paths | Create `/mnt/sfs/extra_model_paths.yaml` |
| Slow cold start | Normal for scale-to-zero | First request takes 1-2 min, subsequent are fast |
| Queue-manager 401 | Missing auth header | Add `SERVERLESS_API_KEY` to .env |

---

**Last Updated:** 2026-02-04 (Phase 11 complete)
