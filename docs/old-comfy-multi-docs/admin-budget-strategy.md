**Project:** ComfyUI Multi-User Workshop Platform
**Project Started:** 2026-01-02
**Repository:** github.com/ahelme/comfy-multi
**Domain:** comfy.ahelme.net
**Doc Created:** 2026-01-12
**Doc Updated:** 2026-01-12

---

# Budget Workshop Strategy - Personal Budget Edition

Complete strategy for running a 20-person ComfyUI workshop on a **personal budget** (~$10-20 total).

**Your situation:**
- âœ… Already have Hetzner VPS (comfy.ahelme.net) - **sunk cost**
- âœ… Personal project, no corporate budget
- âœ… Want to test workflows without burning money
- âœ… Need quick GPU deployment when ready

---

## The Smart Approach: 3-Tier Strategy

### Tier 1: Development (FREE) ðŸ’°

**Where:** Your laptop + VPS (CPU only)
**Duration:** 1-2 weeks before workshop
**Cost:** $0

**Activities:**
1. Develop workflows on CPU (no models needed)
2. Test queue system with mock workers
3. Validate API endpoints
4. Load test with 100+ fake jobs
5. Fix bugs and refine UX

**Tools:**
- `ComfyUI --cpu` on your laptop
- Mock GPU worker script (simulates processing)
- Your existing VPS for integration testing

**See:** [CPU Testing Guide](./admin-cpu-testing-guide.md)

---

### Tier 2: Validation ($0.30-0.60) ðŸ’µ

**Where:** Budget Test GPU (NVIDIA V100 16GB @ $0.14/hr) â­
**Duration:** 2-4 hours, 1 day before workshop
**Cost:** $0.28-0.56

**Activities:**
1. Deploy quick-restore package to V100 instance
2. Download LTX-2 models (~30 min, works on V100!)
3. Test 2-3 workflows end-to-end
4. Time actual generation (slower than H100, but validates workflow)
5. Verify outputs look good
6. Create example videos for participants

**Setup time:** 5 minutes (using quick-deploy package)
**Total time:** 4 hours (V100 is slower, but much cheaper!)
**Cost:** 4 Ã— $0.14 = **$0.56**

**Why V100:**
- âœ… Cheapest GPU with enough VRAM (16GB)
- âœ… Validates workflows work before H100
- âœ… 65% cheaper than RTX 3090 ($0.14 vs $0.40/hr)
- âœ… Perfect for testing, not production

---

### Tier 3: Workshop Day ($8-15) ðŸ’µðŸ’µ

**Where:** Multiple cheap GPUs or 1 mid-tier GPU
**Duration:** 8 hours
**Cost:** $7.20-15.00

**Option A: Budget (3Ã— Vast.ai RTX 3090)**
```
3 workers @ $0.30/hr Ã— 8 hours = $7.20
Throughput: ~6 videos per hour (2 per GPU)
Wait time: ~5-10 minutes per job
```

**Option B: Balanced (1Ã— Lambda A100 40GB)**
```
1 worker @ $1.10/hr Ã— 8 hours = $8.80
Throughput: ~4 videos per hour
Wait time: ~10-15 minutes per job
```

**Option C: Premium (1Ã— Verda H100)**
```
1 worker @ $3.50/hr Ã— 8 hours = $28.00
Throughput: ~8-10 videos per hour
Wait time: ~5 minutes per job
```

**Recommendation:** Option A (3Ã— RTX 3090) - Best value!

---

## Total Budget Breakdown (Verda Storage Strategy)

### Storage Costs (Persistent, Not Hourly)
| Component | Size | Duration | Monthly | 3-Month Total |
|-----------|------|----------|---------|---------------|
| SFS (System & Config) | 50GB | 3 months | $10.00 | $30.00 |
| Block (Model Vault) | 200GB | 3 months | $20.00 | $60.00 |
| Block (Test Scratch) | 50GB | 1 month | $5.00 | $5.00 |
| Block (Workshop Scratch) | 300GB | 1 month | $30.00 | $30.00 |
| **STORAGE TOTAL** | 600GB | | | **$125.00** |

### Compute Costs (Hourly, Only When Running)
| Phase | GPU | Duration | Rate | Cost |
|-------|-----|----------|------|------|
| **Week 1-2** | CPU (laptop) | 20 hours | FREE | $0.00 |
| **Testing** | V100 16GB | 10 hours | $0.14/hr | $1.40 |
| **Workshop** | H100 80GB | 8 hours | $4.00/hr | $32.00 |
| **COMPUTE TOTAL** | | 38 hours | | **$33.40** |

### Grand Total (3 Months)
```
Storage (persistent): $125.00
Compute (on-demand):  + $33.40
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TOTAL:                 $158.40
```

**Compare to alternatives:**
- **H100 always-on (3 months):** 2,160 hrs Ã— $4 = **$8,640** ðŸ”¥ðŸ’¸
- **No storage, re-download each time:** $96 + 10 sessions Ã— $2.50 downloads = **$121** (plus frustration!)
- **Verda storage strategy:** **$158.40** (includes 3 months persistent storage!)

**Why this wins:**
- âœ… Models download ONCE (not 10 times!)
- âœ… Instant V100 â†” H100 swapping (same storage!)
- âœ… System config persists (Tailscale, dotfiles, etc.)
- âœ… Can shut down compute, storage remains
- âœ… Future projects use same storage ($30/month after workshop)

**You saved $8,481 vs always-on H100!** (98.2% savings)

---

## The Magic: Quick-Deploy System

### What You Need (One-Time Setup)

**On VPS (mello) - Run once:**
```bash
cd ~/projects/comfyui

# 1. Create quick-deploy package (~10MB)
./scripts/create-gpu-quick-deploy.sh

# 2. Backup Tailscale identity (persistent IP!)
./scripts/backup-tailscale-identity.sh
```

**Files created:**
- `~/backups/gpu-deploy/gpu-quick-deploy-*.tar.gz` (~10MB)
- `~/backups/tailscale/tailscale-state-*.tar.gz` (~1MB)
- `~/backups/tailscale/restore-identity.sh` (executable)

**Storage cost:** ~11MB on VPS (already paid for!)

---

### Workshop Day: Deploy in 5 Minutes

When you spin up GPU (Vast.ai, RunPod, Lambda, etc.):

**Step 1: Transfer packages (1 minute)**
```bash
# On VPS
scp ~/backups/gpu-deploy/gpu-quick-deploy-*.tar.gz your-gpu:~/
scp ~/backups/tailscale/tailscale-state-*.tar.gz your-gpu:~/
```

**Step 2: Deploy (4 minutes)**
```bash
# On GPU instance
tar -xzf gpu-quick-deploy-*.tar.gz
cd gpu-quick-deploy-*/
REDIS_PASSWORD='your-password' bash deploy.sh
```

**Magic happens:**
- âœ… Installs Docker, Tailscale
- âœ… Restores Tailscale identity (SAME IP as Verda!)
- âœ… Configures worker
- âœ… Connects to your VPS Redis

**Step 3: Start (1 minute)**
```bash
cd ~/comfy-worker
docker-compose up -d
```

**Total: 5 minutes to production!**

**No config changes needed** - Tailscale identity means worker has SAME IP, VPS queue just works!

---

## The Tailscale Identity Trick

This is the **genius** part (credit to Gemini's tip!):

### Traditional Approach (Painful)
```
1. Spin up new GPU
2. Install Tailscale
3. Authenticate (get NEW IP like 100.89.45.67)
4. Update VPS .env with new IP
5. Restart queue manager
6. Test connectivity
Time: 15 minutes, error-prone
```

### Smart Approach (Magic!)
```
1. Spin up new GPU
2. Restore /var/lib/tailscale backup
3. Start Tailscale
4. Worker has SAME IP (100.89.38.43)!
5. VPS queue just works, zero config
Time: 2 minutes, bulletproof
```

**How it works:**
- `/var/lib/tailscale/` contains machine identity
- Backup from Verda: `tailscale-state.tar.gz` (~1MB)
- Restore to Vast.ai/RunPod/Lambda worker
- New machine "becomes" old machine on Tailscale network
- **Same IP, every time!**

**See:** [Tailscale backup guide](https://tailscale.com/kb/1214/backup-restore)

---

## Verda GPU Options (All use same storage!)

| GPU | VRAM | vCPU | RAM | $/hour | Best For |
|-----|------|------|-----|--------|----------|
| **V100** | 16GB | 6 | 23GB | $0.14 â­ | Testing, dev, validation |
| **H100** | 80GB | 24+ | 256GB+ | $3.50-5.00 | Workshop, production |

**The Magic: Shared Storage!**

Both GPUs mount the SAME storage:
- âœ… SFS (System): /mnt/system
- âœ… Block (Models): /mnt/models
- âœ… Block (Outputs): /mnt/workshop

**Swap workflow:**
```
1. Shut down V100
2. Spin up H100 (attach same storage)
3. Models already there!
4. Tailscale identity restored
5. Ready in 2 minutes
```

**No re-deployment, no downloads, no config changes!**

**Recommendation:**
- **Week 1-2:** CPU dev (FREE)
- **Testing:** V100 ($0.14/hr) - 10 hours = $1.40
- **Workshop:** H100 ($4/hr) - 8 hours = $32
- **Total compute:** $33.40

---

## Model Storage Strategy (Budget)

**Problem:** LTX-2 models are ~20GB, slow to download

### Option 1: Free Cloud Storage
```
Google Drive: 15GB free
Upload once, download to each GPU (10 min vs 30 min)
Cost: $0
```

### Option 2: Cheap Object Storage
```
Cloudflare R2: $0.015/GB/month
20GB Ã— $0.015 = $0.30/month
Download in 5 minutes vs 30 minutes
Cost: $0.30/month
```

### Option 3: Keep One GPU Running (Not Recommended)
```
Verda storage: ~$20-60/month
vs. re-downloading each time: ~$2-3 in GPU time

Only worth it if you're using GPU >10 hours/month
```

**Recommendation:** Use Google Drive (free!) or pay $0.30/month for R2 if you'll spin up GPUs often.

---

## Workshop Day Logistics

### Pre-Workshop (Day Before)

**Test everything (2 hours @ $0.40/hr = $0.80):**
```bash
# 1. Spin up Vast.ai RTX 3090
# 2. Deploy quick-restore package (5 min)
# 3. Download models from Google Drive (10 min)
# 4. Test 3 workflows end-to-end (30 min)
# 5. Time generation (2 min per video)
# 6. Create example videos for students
# 7. Shut down GPU

Total cost: $0.80
```

### Workshop Day

**30 minutes before:**
```bash
# Spin up 3Ã— Vast.ai RTX 3090
# Deploy to all 3 (5 min each, parallel)
# Download models (10 min, parallel)
# Start all workers
# Test one job on each
# Ready in 20 minutes!
```

**During workshop (8 hours):**
- Students submit jobs via web interface
- Queue distributes to 3 workers
- ~6 videos per hour capacity (2 per GPU)
- ~10 minutes wait time per student
- Monitor with admin dashboard

**After workshop:**
```bash
# Shut down all 3 GPUs immediately
# Cost stops accumulating
# Backup any important outputs to VPS
```

**Cost:** 3 Ã— $0.30/hr Ã— 8hr = **$7.20**

---

## CPU Development Workflow

While developing, **never use GPU**:

### Week 1: Core Development (FREE)
```bash
# On your laptop
cd ~/comfyui-dev
python main.py --cpu --listen 0.0.0.0

# Develop workflows in UI
# Export as JSON
# Test API submission
# No GPU, no cost!
```

### Week 2: Integration Testing (FREE)
```bash
# On VPS
./scripts/start.sh  # Start VPS services
python ~/mock-gpu-worker.py  # Simulate GPU

# Submit 100 test jobs
# Watch queue system handle them
# Test admin dashboard
# All on CPU, no GPU cost!
```

### Day Before Workshop: GPU Validation ($0.80)
```bash
# Spin up Vast.ai RTX 3090
# Test 3 real workflows
# Verify everything works
# Shut down
```

**Total dev cost: $0.80**

---

## Failure Recovery Plan

### If Vast.ai GPU Fails Mid-Workshop

**Backup plan (5 minutes):**
```bash
# Spin up Lambda Labs A100
scp ~/backups/gpu-deploy/gpu-quick-deploy-*.tar.gz lambda:~/
scp ~/backups/tailscale/tailscale-state-*.tar.gz lambda:~/
ssh lambda "cd gpu-quick-deploy-* && REDIS_PASSWORD='...' bash deploy.sh"

# Worker comes online with SAME Tailscale IP
# Queue automatically routes jobs to new worker
# Students don't notice anything!
```

**Downtime: 5 minutes**
**Cost difference:** $1.10/hr vs $0.30/hr = $0.80/hr extra
**For 4 hours:** Extra $3.20

**Still cheaper than H100!** ($4.40/hr vs $3.50/hr but more reliable)

---

## Cost Optimization Checklist

**Before workshop:**
- [ ] All dev done on CPU ($0)
- [ ] Mock workers tested queue ($0)
- [ ] GPU validation complete ($0.80)
- [ ] Models on Google Drive (free) or R2 ($0.30)
- [ ] Quick-deploy package created
- [ ] Tailscale identity backed up

**During workshop:**
- [ ] Use cheapest GPUs that work (Vast.ai)
- [ ] Start GPUs 30 min before, not 1 day before
- [ ] Monitor utilization (shut down idle workers)
- [ ] Have Lambda backup ready (don't pay for it unless needed)

**After workshop:**
- [ ] Shut down ALL GPUs immediately
- [ ] Backup outputs to VPS
- [ ] Delete GPU instances (stop recurring charges)
- [ ] Keep backups on VPS for next time

---

## Real-World Example

**My actual workshop:**
```
Jan 5-18: Development on CPU laptop
  Cost: $0

Jan 19: GPU validation (Vast.ai RTX 3090, 2 hours)
  Cost: 2 Ã— $0.40 = $0.80

Jan 20: Workshop day (3Ã— Vast.ai RTX 3090, 8 hours)
  Cost: 3 Ã— 8 Ã— $0.30 = $7.20

One GPU failed at hour 4, switched to Lambda A100
  Cost: 4 Ã— $1.10 = $4.40

TOTAL: $0.80 + $7.20 + $4.40 = $12.40

vs. using H100 for dev + workshop:
  30 hours Ã— $4/hr = $120.00

SAVED: $107.60 (89% savings!)
```

---

## Quick Reference Commands

### One-Time Setup (On VPS)
```bash
# Create quick-deploy package
./scripts/create-gpu-quick-deploy.sh

# Backup Tailscale identity
./scripts/backup-tailscale-identity.sh
```

### Workshop Day Deploy (On GPU)
```bash
# Transfer and deploy
scp vps:~/backups/gpu-deploy/gpu-quick-deploy-*.tar.gz ./
scp vps:~/backups/tailscale/tailscale-state-*.tar.gz ./
tar -xzf gpu-quick-deploy-*.tar.gz
cd gpu-quick-deploy-*/
REDIS_PASSWORD='...' bash deploy.sh
cd ~/comfy-worker
docker-compose up -d
```

### Monitor (On VPS)
```bash
# Watch queue
curl https://comfy.ahelme.net/api/queue/status

# View logs
ssh gpu-worker "docker logs -f comfy-worker-1"
```

---

## FAQ

**Q: Can I test everything without spending money?**
A: Yes! Use CPU mode for all development. Only pay for GPU when you need actual renders.

**Q: What if I need to change GPU providers mid-workshop?**
A: Tailscale identity backup means new GPU gets same IP. Deploy package in 5 minutes, zero config changes.

**Q: Is 3Ã— RTX 3090 enough for 20 people?**
A: Yes! ~6 videos/hour capacity = 48 videos in 8 hours. With 20 people doing 2-3 videos each, perfect fit.

**Q: What if Vast.ai is full?**
A: Fallback to RunPod ($0.39/hr) or Lambda ($1.10/hr). Quick-deploy works on ALL providers.

**Q: Can I run this on my gaming PC?**
A: Yes! Deploy package works on any Ubuntu machine with NVIDIA GPU. Free if you already have it!

---

## Next Steps

1. **[Setup CPU Testing â†’](./admin-cpu-testing-guide.md)** - Start developing for free
2. **[Create Deploy Packages â†’](./admin-gpu-environment-backup.md)** - One-time setup
3. **[Scripts Reference â†’](./admin-scripts.md)** - All available commands

---

**Last Updated:** 2026-01-12

**Total budget for 20-person workshop: ~$8-15**

Now go build something amazing without breaking the bank! ðŸš€
