# ComfyuME Multi-User Workshop Platform Configuration
# App Version: comfyume v0.11.0 (for ComfyUI v0.11.0)
# Copy this file to .env and update with your values
# For production: Use consolidated .env from comfymulti-scripts repo
# VERSION: 0.3.5
# UPDATED: 2026-02-07

# =============================================================================
# SCRIPT METADATA
# =============================================================================
ENV_VERSION="0.3.5"
ENV_DATE="2026-02-07"
PROJECT_NAME=comfyume

# ============================================================================
# APP CONFIGURATION
# ============================================================================

# -----------------------------------------------------------------------------
# SERVER MODE SELECTION
# -----------------------------------------------------------------------------
# Set to 'single' for single server deployment
# Set to 'dual' for dual server deployment (App Server + Inference Server)
SERVER_MODE=dual

# -----------------------------------------------------------------------------
# APP-SERVER: NGINX MODE SELECTION
# -----------------------------------------------------------------------------
# Set to 'true' to use host nginx, 'false' to use Docker nginx container
USE_HOST_NGINX=true

# -----------------------------------------------------------------------------
# APP-SERVER: NGINX CONFIGURATION
# -----------------------------------------------------------------------------
NGINX_HTTP_PORT=80
NGINX_HTTPS_PORT=443
# Client max body size (for uploads) note: models/video files are large!
NGINX_CLIENT_MAX_BODY_SIZE=10000M

# -----------------------------------------------------------------------------
# COMFYUI CONFIGURATION
# -----------------------------------------------------------------------------
# ComfyUI version (v0.11.0 for comfyume - pinned in Dockerfiles)
# Current: v0.11.0 (stable for comfyume rebuild)
# Latest: Check https://github.com/comfyanonymous/ComfyUI/releases
# NOTE: Do NOT use "latest" - causes deployment issues
COMFYUI_VERSION=v0.11.0

# ComfyUI deployment mode (clarifies architecture intent)
# - frontend-testing: UI only, no inference (uses --cpu flag internally)
# - worker: Full inference capability with GPU (no --cpu flag)
COMFYUI_MODE=frontend-testing

# ComfyUI listen port (internal)
COMFYUI_PORT=8188

# -----------------------------------------------------------------------------
# DEVELOPMENT / DEBUGGING
# -----------------------------------------------------------------------------
DEBUG=false
VERBOSE_LOGS=false
LOG_LEVEL=INFO

# -----------------------------------------------------------------------------
# APP-SERVER:  DOMAIN & SSL CONFIGURATION
# -----------------------------------------------------------------------------
DOMAIN=workshop.example.com  # Production: aiworkshop.art
# SSL certificate paths (update with your paths)
SSL_CERT_PATH=/path/to/fullchain.pem
SSL_KEY_PATH=/path/to/privkey.pem

# -----------------------------------------------------------------------------
# APP-SERVER:  QUEUE MANAGER SERVICE
# -----------------------------------------------------------------------------
QUEUE_MANAGER_HOST=queue-manager
QUEUE_MANAGER_PORT=3000
QUEUE_MANAGER_LOG_LEVEL=INFO

QUEUE_MODE=fifo                 # or round_robin, priority
ENABLE_PRIORITY=true            # Allow instructor override
JOB_TIMEOUT=3600                # 1 hour max per job (seconds)
MAX_QUEUE_DEPTH=100             # 0 = unlimited

# -----------------------------------------------------------------------------
# REDIS - Connections to queue
# -----------------------------------------------------------------------------
# For dual-server deployment:
# - APP_SERVER_REDIS_HOST: Use 'redis' (Docker network) on app server
# - INFERENCE_SERVER_REDIS_HOST: Use Tailscale IP or domain on inference workers
# For single-server deployment:
# - APP_SERVER_REDIS_HOST=redis
# - INFERENCE_SERVER_REDIS_HOST=redis

APP_SERVER_REDIS_HOST=redis
APP_SERVER_REDIS_PORT=6379
REDIS_BIND_IP=127.0.0.1
INFERENCE_SERVER_REDIS_HOST=workshop.example.com

REDIS_PASSWORD=changeme_secure_password
REDIS_PERSISTENCE=yes

# -----------------------------------------------------------------------------
# APP-SERVER: USER CONFIGURATION
# -----------------------------------------------------------------------------
NUM_USERS=20
USER_ID_PREFIX=user
USER_CREDENTIALS_FORMAT="username:password"

# -----------------------------------------------------------------------------
# COMFYUME ADMIN DASHBOARD
# -----------------------------------------------------------------------------
ADMIN_PORT=8080
ADMIN_USERNAME=admin
ADMIN_PASSWORD=changeme_admin_password

# Isolate mode: start with all API endpoints disabled (fault isolation)
# ADMIN_ISOLATE_MODE=false

# -----------------------------------------------------------------------------
# AUTH COOKIE PERSISTENCE (#45)
# -----------------------------------------------------------------------------
# Set a random secret to enable 24h session cookies (reduces Basic Auth re-prompts)
# Generate with: openssl rand -base64 32 | tr -d '+/=' | head -c 32
# Leave unset to use standard Basic Auth only
# AUTH_COOKIE_SECRET=

# -----------------------------------------------------------------------------
# STORAGE PATHS
# -----------------------------------------------------------------------------
# Paths are relative to docker-compose.yml location
MODELS_PATH=./data/models
OUTPUTS_PATH=./data/outputs
INPUTS_PATH=./data/inputs
WORKFLOWS_PATH=./data/workflows

# -----------------------------------------------------------------------------
# MODEL DOWNLOADS (Admin Panel)
# -----------------------------------------------------------------------------
# HuggingFace token for gated models (optional)
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# ntfy.sh topic for download notifications (optional)
# NTFY_TOPIC=comfyume-downloads

# -----------------------------------------------------------------------------
# INFERENCE MODE
# -----------------------------------------------------------------------------
# "local"      - GPU workers on same or remote instance (legacy)
# "redis"      - Workers poll Redis queue (legacy)
# "serverless" - Direct HTTP to serverless containers (current production)
INFERENCE_MODE=local

# -----------------------------------------------------------------------------
# SERVERLESS INFERENCE (when INFERENCE_MODE=serverless)
# -----------------------------------------------------------------------------
# Active endpoint selector:
#   "default"        - Uses SERVERLESS_ENDPOINT fallback
#   "h200-spot"      - Verda (ex. DataCrunch) H200 141GB spot (~EUR 0.97/hr)
#   "h200-on-demand" - Verda H200 141GB on-demand (~EUR 2.80/hr)
#   "b300-spot"      - Verda B300 288GB spot (~EUR 1.61/hr)
#   "b300-on-demand" - Verda B300 288GB on-demand (~EUR 4.63/hr)
SERVERLESS_ACTIVE=default

# Endpoint URLs (set the ones you have deployed)
# SERVERLESS_ENDPOINT=                        # Fallback endpoint
# SERVERLESS_ENDPOINT_H200_SPOT=
# SERVERLESS_ENDPOINT_H200_ON_DEMAND=
# SERVERLESS_ENDPOINT_B300_SPOT=
# SERVERLESS_ENDPOINT_B300_ON_DEMAND=

# API key for serverless auth (Bearer token)
# SERVERLESS_API_KEY=

# SFS-based result delivery (default for serverless)
# SFS_DELIVERY_ENABLED=true
# SFS_MAX_WAIT=600              # Default timeout (seconds) for admin timeout knob (#85)

# -----------------------------------------------------------------------------
# LOCAL WORKER CONFIGURATION (when INFERENCE_MODE=local or redis)
# -----------------------------------------------------------------------------
NUM_WORKERS=1                   # Number of GPU workers
WORKER_GPU_MEMORY_LIMIT=70G     # H100 has 80GB, leave 10GB for system
WORKER_RESTART_POLICY=unless-stopped

# -----------------------------------------------------------------------------
# WORKER HEARTBEAT
# -----------------------------------------------------------------------------
WORKER_HEARTBEAT_TIMEOUT=60
WORKER_POLL_INTERVAL=2

# ============================================================================
# REQUIRED WORKSHOP MODELS (Download to Remote GPU)
# ============================================================================
# These models should be downloaded to data/models/shared/ on the GPU instance
#
# PRIMARY WORKSHOP MODELS (v0.11.0):
# - Flux.2 Klein (9B & 4B) - Black Forest Labs
# - LTX-2 (19B) - Lightricks
#
# NOTE: H100 80GB can hold 2-3 large models simultaneously.
# Plan model loading based on workshop schedule.

# =============================================================================
# SECRETS (DO NOT COMMIT - for reference only)
# =============================================================================
# The following variables are in the consolidated .env (comfymulti-scripts)
# but should NOT be committed to version control:
#
# VERDA_TAILSCALE_IP
# MELLO_TAILSCALE_IP
# VERDA_DEV_USER_PASSWORD
# PUB_KEY_MELLO
# PUB_KEY_VERDA
# PUB_KEY_AEON
# VERDA_SSH_PRIVATE_KEY_LOCATION
# VERDA_SSH_PRIVATE_KEY
# GH_TOKEN
# GH_APP_REPO
# GH_APP_BRANCH
# GH_SCRIPTS_REPO
# GH_SCRIPTS_BRANCH
# R2_ENDPOINT
# R2_ACCESS_KEY_ID
# R2_SECRET_ACCESS_KEY
# R2_MODELS_BUCKET
# R2_USER_FILES_BUCKET
# R2_WORKER_CONTAINER_TAR_BALL_BUCKET
# R2_CACHE_BUCKET
# UBUNTU_PRO_TOKEN
# USER_CREDENTIALS_USER001-020
